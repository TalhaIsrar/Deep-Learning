{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89360ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import scipy.misc as mis\n",
    "from skimage.transform import resize\n",
    "from skimage import data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2415fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45980f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b922ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[1]*train_set_x_orig.shape[2]*train_set_x_orig.shape[3],train_set_x_orig.shape[0])\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[1]*test_set_x_orig.shape[2]*test_set_x_orig.shape[3],test_set_x_orig.shape[0])\n",
    "train_set_x_flatten = np.concatenate((train_set_x_flatten.T, test_set_x_flatten.T), axis=0).T\n",
    "train_set_y = np.concatenate((train_set_y,test_set_y),axis=1)\n",
    "num_px = train_set_x_orig.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "120d549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the sigmoid activation function for output layer\n",
    "def sigmoid(z):\n",
    "  return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e67e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the parameters for 2 hidden layers\n",
    "def initialize_parameters(n_x, n_h1, n_h2, n_y):\n",
    "  \n",
    "  #We do random assignment to parameters instead of zero\n",
    "  W1 = np.random.randn(n_h1, n_x)\n",
    "  b1 = np.random.randn(n_h1, 1)\n",
    "  W2 = np.random.randn(n_h2, n_h1)\n",
    "  b2 = np.random.randn(n_h2, 1)\n",
    "  W3 = np.random.randn(n_y, n_h2)\n",
    "  b3 = np.random.randn(n_y, 1)\n",
    "    \n",
    "  parameters = {\n",
    "    \"W1\": W1,\n",
    "    \"b1\" : b1,\n",
    "    \"W2\": W2,\n",
    "    \"b2\" : b2,\n",
    "    \"W3\": W3,\n",
    "    \"b3\" : b3\n",
    "  }\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7308efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform forward propagation\n",
    "def forward_prop(X, parameters):\n",
    "  W1 = parameters[\"W1\"]\n",
    "  b1 = parameters[\"b1\"]\n",
    "  W2 = parameters[\"W2\"]\n",
    "  b2 = parameters[\"b2\"]\n",
    "  W3 = parameters[\"W3\"]\n",
    "  b3 = parameters[\"b3\"]\n",
    "    \n",
    "  #b is resized by broadcasting\n",
    "\n",
    "  #tanh is used as activation ftn for hidden layers and sigmoid for output layer\n",
    "  Z1 = np.dot(W1, X) + b1\n",
    "  A1 = np.tanh(Z1)\n",
    "  Z2 = np.dot(W2, A1) + b2\n",
    "  A2 = np.tanh(Z2)    \n",
    "  Z3 = np.dot(W3, A2) + b3\n",
    "  A3 = sigmoid(Z3)\n",
    "  \n",
    "  #cache is used in back propagation\n",
    "  cache = {\n",
    "    \"A1\": A1,\n",
    "    \"A2\": A2,\n",
    "    \"A3\": A3  \n",
    "  }\n",
    "  return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0aad284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the loss function to retune the parameters\n",
    "def calculate_cost(A3, Y):\n",
    "    \n",
    "  #cost ftn of logisitic regression\n",
    "  cost = -np.sum(np.multiply(Y, np.log(A3)) +  np.multiply(1-Y, np.log(1-A3)))/m\n",
    "  cost = np.squeeze(cost)\n",
    "\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e16e9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform back propagation\n",
    "def backward_prop(X, Y, cache, parameters):\n",
    "  A1 = cache[\"A1\"]\n",
    "  A2 = cache[\"A2\"]\n",
    "  A3 = cache[\"A3\"]\n",
    "\n",
    "  W3 = parameters[\"W3\"]\n",
    "  W2 = parameters[\"W2\"]\n",
    "\n",
    "  #backpropagation for output layer\n",
    "  dZ3 = A3 - Y\n",
    "  dW3 = np.dot(dZ3, A2.T)/m\n",
    "  db3 = np.sum(dZ3, axis=1, keepdims=True)/m\n",
    "\n",
    "  #backpropagation for 2nd hidden layer\n",
    "  dZ2 = np.multiply(np.dot(W3.T, dZ3), 1-np.power(A2, 2))\n",
    "  dW2 = np.dot(dZ2, A1.T)/m\n",
    "  db2 = np.sum(dZ2, axis=1, keepdims=True)/m\n",
    "    \n",
    "  #backpropagation for 1st hidden layer \n",
    "  dZ1 = np.multiply(np.dot(W2.T, dZ2), 1-np.power(A1, 2))\n",
    "  dW1 = np.dot(dZ1, X.T)/m\n",
    "  db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n",
    "\n",
    "  grads = {\n",
    "    \"dW1\": dW1,\n",
    "    \"db1\": db1,\n",
    "    \"dW2\": dW2,\n",
    "    \"db2\": db2,\n",
    "    \"dW3\": dW3,\n",
    "    \"db3\": db3\n",
    "  }\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2b996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We update the parameters\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "  W1 = parameters[\"W1\"]\n",
    "  b1 = parameters[\"b1\"]\n",
    "  W2 = parameters[\"W2\"]\n",
    "  b2 = parameters[\"b2\"]\n",
    "  W3 = parameters[\"W3\"]\n",
    "  b3 = parameters[\"b3\"]\n",
    "    \n",
    "  dW1 = grads[\"dW1\"]\n",
    "  db1 = grads[\"db1\"]\n",
    "  dW2 = grads[\"dW2\"]\n",
    "  db2 = grads[\"db2\"]\n",
    "  dW3 = grads[\"dW3\"]\n",
    "  db3 = grads[\"db3\"]\n",
    "    \n",
    "  #performing gradient descenet\n",
    "  W1 = W1 - learning_rate * dW1\n",
    "  b1 = b1 - learning_rate * db1\n",
    "  W2 = W2 - learning_rate * dW2\n",
    "  b2 = b2 - learning_rate * db2\n",
    "  W3 = W3 - learning_rate * dW3\n",
    "  b3 = b3 - learning_rate * db3\n",
    "\n",
    "  new_parameters = {\n",
    "    \"W1\": W1,\n",
    "    \"W2\": W2,\n",
    "    \"W3\": W3,\n",
    "    \"b1\" : b1,\n",
    "    \"b2\" : b2,\n",
    "    \"b3\" : b3\n",
    "  }\n",
    "\n",
    "  return new_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62990e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model to run neural network\n",
    "def model(X, Y, n_x, n_h1, n_h2, n_y, num_of_iters, learning_rate):\n",
    "  parameters = initialize_parameters(n_x, n_h1, n_h2, n_y)\n",
    "\n",
    "  #Running the model for a number of iterations\n",
    "  for i in range(0, num_of_iters+1):\n",
    "    a2, cache = forward_prop(X, parameters)\n",
    "\n",
    "    cost = calculate_cost(a2, Y)\n",
    "\n",
    "    grads = backward_prop(X, Y, cache, parameters)\n",
    "\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "    if(i % 100 == 0):\n",
    "      print('Cost after iteration# {:d}: {:f}'.format(i, cost))\n",
    "\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eecd8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for prediction\n",
    "def predict(X, parameters):\n",
    "  a2, cache = forward_prop(X, parameters)\n",
    "  yhat = a2\n",
    "  yhat = np.squeeze(yhat)\n",
    "\n",
    "  #If prediction is more than 0.5 we classify as 1 otherwise as 0\n",
    "  if(yhat >= 0.5):\n",
    "    y_predict = 1\n",
    "  else:\n",
    "    y_predict = 0\n",
    "\n",
    "  return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb4f5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration# 0: 2.526776\n",
      "Cost after iteration# 100: 0.354094\n",
      "Cost after iteration# 200: 0.222968\n",
      "Cost after iteration# 300: 0.150946\n",
      "Cost after iteration# 400: 0.119052\n",
      "Cost after iteration# 500: 0.099429\n",
      "Cost after iteration# 600: 0.071215\n",
      "Cost after iteration# 700: 0.056335\n",
      "Cost after iteration# 800: 0.042474\n",
      "Cost after iteration# 900: 0.036121\n",
      "Cost after iteration# 1000: 0.032231\n",
      "Cost after iteration# 1100: 0.029257\n",
      "Cost after iteration# 1200: 0.025110\n",
      "Cost after iteration# 1300: 0.022686\n",
      "Cost after iteration# 1400: 0.020830\n",
      "Cost after iteration# 1500: 0.019270\n",
      "Cost after iteration# 1600: 0.017981\n",
      "Cost after iteration# 1700: 0.016599\n",
      "Cost after iteration# 1800: 0.014589\n",
      "Cost after iteration# 1900: 0.013484\n",
      "Cost after iteration# 2000: 0.012676\n"
     ]
    }
   ],
   "source": [
    "#main code starts here\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(2)\n",
    "\n",
    "    X = train_set_x_flatten\n",
    "    Y = train_set_y\n",
    "    \n",
    "    m = X.shape[1]\n",
    "\n",
    "    # Set the hyperparameters\n",
    "    n_x = 12288      #No. of neurons in first layer\n",
    "    n_h1 = 192     #No. of neurons in 1st hidden layer\n",
    "    n_h2 = 32     #No. of neurons in 2nd hidden layer\n",
    "    n_y = 1      #No. of neurons in output layer\n",
    "    num_of_iters = 2000\n",
    "    learning_rate = 0.5\n",
    "    \n",
    "    trained_parameters = model(X, Y, n_x, n_h1, n_h2, n_y, num_of_iters, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ea83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 14 \n",
      "14 11\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for i in range(1,51):\n",
    "    my_image = \"test\" + str(i) + \".jpg\"\n",
    "    fname = \"images/\" + my_image\n",
    "    img= Image.open(fname)\n",
    "    np_img = np.array(img)\n",
    "    image = np_img/255.\n",
    "    my_image = resize(image, (num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "    y_predict = int(predict(my_image, trained_parameters))\n",
    "    y_hat = 0\n",
    "    if (i < 26):\n",
    "        y_hat = 1\n",
    "\n",
    "    if ((y_hat == 1) & (y_predict == 1)):\n",
    "        tp = tp + 1\n",
    "    \n",
    "    else: \n",
    "        if ((y_hat == 0) & (y_predict == 0)):\n",
    "            tn = tn + 1\n",
    "\n",
    "        else: \n",
    "            if ((y_hat == 1) & (y_predict == 0)):\n",
    "                fn = fn + 1\n",
    "\n",
    "            else: \n",
    "                if ((y_hat == 0) & (y_predict == 1)):\n",
    "                    fp = fp + 1    \n",
    "\n",
    "print(\"{:d} {:d} \\n{:d} {:d}\" .format(tp,fp,fn,tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394784e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f018e199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "sum = 0\n",
    "for i in range(len(test_set_x_flatten.T)):\n",
    "    y_predict = predict(test_set_x_flatten.T[i].reshape((test_set_x_flatten.shape[0]),1), trained_parameters)\n",
    "    y_hat = test_set_y.T[i][0]\n",
    "    if (y_predict == y_hat):\n",
    "        sum = sum + 1\n",
    "    total = total + 1\n",
    "    \n",
    "print(sum*100/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e7984fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "sum = 0\n",
    "for i in range(len(train_set_x_flatten.T)):\n",
    "    y_predict = predict(train_set_x_flatten.T[i].reshape((train_set_x_flatten.shape[0]),1), trained_parameters)\n",
    "    y_hat = train_set_y.T[i][0]\n",
    "    if (y_predict == y_hat):\n",
    "        sum = sum + 1\n",
    "    total = total + 1\n",
    "    \n",
    "print(sum*100/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41f7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe35f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
